{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOyWHaIuZ/gX7rLmxutJQqF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Humba01/imersaoia-alura-project/blob/main/imersaoia_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Projeto para a avaliação - Imersão IA Alura + Google\n",
        "\n",
        "### Professor/Assistente de línguas estrangeiras digital\n",
        "\n",
        "<div align=\"justify\">\n",
        "\n",
        "O objetivo dessa iniciativa experimental é incetivar pessoas a estudarem novos idiomas (os suportados pela IA do Google nessa ocasião), utilizando conhecimentos sobre LLMs, a API do Gemini e engenharia de prompt para construir-lo. Onde ao colocar entradas de imagem, vídeo e audio e depois os de texto, o Gemini em sua versão 1.0 Pro entregará como saída uma explicação satisfatória sob a entrada coloca pelo usuário. Dando tanto explicações de gramática e léxica, como também aspectos socioculturais e as referências com os dados encontrados.\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "id": "_5JYcHnYIqFo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalando o SDK do Google\n",
        "\n",
        "*Necessário para o funcionamento e manipulação do Gemini API*"
      ],
      "metadata": {
        "id": "UJaKGS9RIOKq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mqQDIb0pB8wh"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definindo importações e adicionando API_KEY para a configuração do Gemini\n",
        "\n",
        "*Primeiras estruturas necessárias para o funcionamento do nosso uso da API por meio do uso da linguagem Python*"
      ],
      "metadata": {
        "id": "klKX7YlKIUxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai"
      ],
      "metadata": {
        "id": "Vs0tnsfqCaPY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "API_KEY = userdata.get(\"SECRET_KEY\")\n",
        "genai.configure(api_key=API_KEY)"
      ],
      "metadata": {
        "id": "GZZFINwACu-u"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definição de configurações de geração das respostas\n",
        "\n",
        "*Define a quantidade de respostas que o LLM do Google retornará, o quão criativo ele pode performar e o número máximo de tokens gerados na saída como resposta*"
      ],
      "metadata": {
        "id": "SEy2hE2iIhOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GENERATION_CONFIG = {\n",
        "  \"candidate_count\": 1,\n",
        "  \"temperature\": 2,\n",
        "  \"top_k\": 1,\n",
        "  \"top_p\": 0,\n",
        "  \"max_output_tokens\": 40000\n",
        "}"
      ],
      "metadata": {
        "id": "lU5AiijsHFve"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definição das configurações de segurança\n",
        "\n",
        "*Define os parâmetros de segurança de resposta sob os conteúdos gerados, nesse caso bloqueando alguns tipos de inputs maliciosos.*"
      ],
      "metadata": {
        "id": "uvd2VGwEI5Tl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SAFETY_SETTINGS = {\n",
        "    \"HARASSMENT\": \"BLOCK_ONLY_HIGH\",\n",
        "    \"HATE\": \"BLOCK_ONLY_HIGH\",\n",
        "    \"SEXUAL\": \"BLOCK_ONLY_HIGH\",\n",
        "    \"DANGEROUS\": \"BLOCK_ONLY_HIGH\"\n",
        "}"
      ],
      "metadata": {
        "id": "S7fKMLS9IEJp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definindo o modelo utilizado\n"
      ],
      "metadata": {
        "id": "eVlPrOK6S78X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(model_name=\"gemini-1.0-pro\",\n",
        "                              generation_config=GENERATION_CONFIG,\n",
        "                              safety_settings=SAFETY_SETTINGS)"
      ],
      "metadata": {
        "id": "IKgmaI5FTBfD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construindo aplicação\n",
        "\n",
        "### Construção do chat\n",
        "\n",
        "*Aqui definimos o chat e como ele será formatado*\n",
        "\n",
        "### Construção da lógica funcional\n",
        "\n",
        "*Implementação lógica de como o assistente de estudos funcionará*\n"
      ],
      "metadata": {
        "id": "tIko0-QYVJAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = model.start_chat(history=[])"
      ],
      "metadata": {
        "id": "n5nh7rCTXzBx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '>', predicate=lambda _: True))"
      ],
      "metadata": {
        "id": "KlPNSnMNbAod"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comandos com o caminho de imagem, vídeo, áudio e texto\n",
        "ARQUIVO_DE_MIDIA = input(\"Insira o caminho de arquivo de imagem, vídeo ou audio:\\n\")\n",
        "PROMPT = input(\"Insira um comando de texto:\\n\")\n",
        "\n",
        "MESSAGE_TEXT = \"Fale qual é a língua utilizada. Explique com detalhes, os significados das expressões, palavras e outros, caso exista. Coloque também contexto histórico, aspectos geográficos caso seja uma localidade, sobre a economia, curiosidades, use tabelas e tópicos de lista caso seja necessário. E as referências com links funcionais:> \"+\"'\"+PROMPT+\"'\"\n",
        "\n",
        "while ARQUIVO_DE_MIDIA != \"fim\" or PROMPT != \"fim\":\n",
        "  if ARQUIVO_DE_MIDIA == \"fim\":\n",
        "    response = chat.send_message(MESSAGE_TEXT)\n",
        "  else:\n",
        "    response = chat.send_message(genai.upload_file(ARQUIVO_DE_MIDIA), MESSAGE_TEXT)\n",
        "  for message in chat.history:\n",
        "    display(to_markdown(f'**{message.role}**: {message.parts[0].text}'))\n",
        "    print('\\n\\n__________________________________________________\\n\\n')\n",
        "  ARQUIVO_DE_MIDIA = input(\"Insira o caminho de arquivo de imagem, vídeo ou audio:\\n\")\n",
        "  PROMPT = input(\"Insira um comando de texto:\\n\")\n",
        "\n",
        "chat.history=[]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHYCsFaSVUxE",
        "outputId": "7837f11d-8259-4de1-e1d2-d461fcef448d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Insira o caminho de arquivo de imagem, vídeo ou audio:\n",
            "fim\n",
            "Insira um comando de texto:\n",
            "fim\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Observações\n",
        "\n",
        "Durante o desenvolvimento do projeto foi planejado o uso de um output na cor azul para representar a resposta do modelo, mas não foi possível colocar na versão final."
      ],
      "metadata": {
        "id": "0kn36CtDoHD8"
      }
    }
  ]
}