{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRPxytlFCjaYxp4QL+8crx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Humba01/imersaoia-alura-project/blob/main/imersaoia_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Projeto para a avaliação - Imersão IA Alura + Google\n",
        "\n",
        "### Professor/Assistente de línguas estrangeiras digital\n",
        "\n",
        "O objetivo dessa iniciativa experimental é incetivar pessoas a estudarem novos idiomas (os suportados pela IA do Google nessa ocasião), utilizando conhecimentos sobre LLMs, a API do Gemini e engenharia de prompt. Onde ao colocar uma entrada tanto de áudio, texto, vídeo ou imagem, o Gemini em sua versão 1.0 Pro entregará como saída uma explicação satisfatória sob a entrada coloca pelo usuário."
      ],
      "metadata": {
        "id": "_5JYcHnYIqFo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalando o SDK do Google\n",
        "\n",
        "*Necessário para o funcionamento e manipulação do Gemini API*"
      ],
      "metadata": {
        "id": "UJaKGS9RIOKq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mqQDIb0pB8wh"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definindo importações e adicionando API_KEY para a configuração do Gemini\n",
        "\n",
        "*Primeiras estruturas necessárias para o funcionamento do nosso uso da API por meio do uso da linguagem Python*"
      ],
      "metadata": {
        "id": "klKX7YlKIUxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai"
      ],
      "metadata": {
        "id": "Vs0tnsfqCaPY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "API_KEY = userdata.get(\"SECRET_KEY\")\n",
        "genai.configure(api_key=API_KEY)"
      ],
      "metadata": {
        "id": "GZZFINwACu-u"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definição de configurações de geração das respostas\n",
        "\n",
        "*Define a quantidade de respostas que o LLM do Google retornará, o quão criativo ele pode performar e o número máximo de tokens gerados na saída como resposta*"
      ],
      "metadata": {
        "id": "SEy2hE2iIhOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GENERATION_CONFIG = {\n",
        "  \"candidate_count\": 1,\n",
        "  \"temperature\": 2,\n",
        "  \"top_k\": 1,\n",
        "  \"top_p\": 0,\n",
        "  \"max_output_tokens\": 40000\n",
        "}"
      ],
      "metadata": {
        "id": "lU5AiijsHFve"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definição das configurações de segurança\n",
        "\n",
        "*Define os parâmetros de segurança de resposta sob os conteúdos gerados, nesse caso removendo a grande parte das respostas com caráter perigoso, discurso de ódio e outros*"
      ],
      "metadata": {
        "id": "uvd2VGwEI5Tl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SAFETY_SETTINGS = {\n",
        "    \"HARASSMENT\": \"BLOCK_LOW_AND_ABOVE\",\n",
        "    \"HATE\": \"BLOCK_LOW_AND_ABOVE\",\n",
        "    \"SEXUAL\": \"BLOCK_LOW_AND_ABOVE\",\n",
        "    \"DANGEROUS\": \"BLOCK_LOW_AND_ABOVE\"\n",
        "}"
      ],
      "metadata": {
        "id": "S7fKMLS9IEJp"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definindo o modelo utilizado\n"
      ],
      "metadata": {
        "id": "eVlPrOK6S78X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(model_name=\"gemini-1.0-pro\",\n",
        "                              generation_config=GENERATION_CONFIG,\n",
        "                              safety_settings=SAFETY_SETTINGS)"
      ],
      "metadata": {
        "id": "IKgmaI5FTBfD"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construindo aplicação\n",
        "\n",
        "### Construção do chat\n",
        "\n",
        "*Aqui definimos o chat e como ele será formatado*\n",
        "\n",
        "### Construção da lógica funcional\n",
        "\n",
        "*Implementação lógica de como o assistente de estudos funcionará*\n"
      ],
      "metadata": {
        "id": "tIko0-QYVJAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = model.start_chat(history=[])"
      ],
      "metadata": {
        "id": "n5nh7rCTXzBx"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '>', predicate=lambda _: True))"
      ],
      "metadata": {
        "id": "KlPNSnMNbAod"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = input(\"Insira um comando:\\n\")\n",
        "\n",
        "while prompt != \"fim\":\n",
        "  response = chat.send_message(\"Explique com detalhes, significado das expressões, palavras e outros, sobre as entradas, podendo usar métodos didáticos, como tabelas expressões matemáticas e outros. Em circunstâncias de temas geopolíticos, históricos, geográficos e econômicos dê um contexto histórico. Como artefatos de resposta, caso queira. Coloque referências com links, não coloque links defeituosos:> \"+\"'\"+prompt+\"'\")\n",
        "  for message in chat.history:\n",
        "    display(to_markdown(f'**{message.role}**: {message.parts[0].text}'))\n",
        "    print('\\n\\n__________________________________________________\\n\\n')\n",
        "  prompt = input(\"Insira um comando:\\n\")\n",
        "\n",
        "chat.history=[]"
      ],
      "metadata": {
        "id": "gHYCsFaSVUxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Observações\n",
        "\n",
        "Durante o desenvolvimento do projeto foi planejado o uso de um output na cor azul para representar a resposta do modelo, mas não foi possível colocar na versão final."
      ],
      "metadata": {
        "id": "0kn36CtDoHD8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testes adicionais"
      ],
      "metadata": {
        "id": "jjbqfjuIrScq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "genai.get_file(\"data_input/image.png\")"
      ],
      "metadata": {
        "id": "sKQr-9DCrV0L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}